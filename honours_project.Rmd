---
title: "Honours Project"
author: "Philo Tchokonte-Nana"
date: '2022-07-26'
output: html_document
---

```{r setup, include=FALSE}
pacman::p_load(pacman, tidyverse, janitor, tidymodels, knitr, missForest, splitstackshape, naniar, doParallel, foreach, furrr, future, themis)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
```

## Data Preparation
```{r prep}
my_file <- "my_dataset.csv"
my_data <- read.csv(my_file,header=TRUE,sep=",") %>% janitor::clean_names(.) %>% rename(.,hiv_status=hi_vstatus) %>% rename(.,TB_status=final_class) 
my_data <- arrange(my_data, barcode)
my_metadata <- "metadata.csv"
metadata <- read.csv(my_metadata,header=TRUE,sep=",") %>% janitor::clean_names(.) %>% rename(.,hiv_status=hi_vstatus) 
metadata <- semi_join(metadata,my_data,by="barcode")
metadata <- arrange(metadata,barcode)

for (i in 9:30) {
  my_data[which(metadata[i] == "OOR <"),i] <- min(my_data[i], na.rm = TRUE)
  my_data[which(metadata[i] == "OOR >"),i] <- max(my_data[i], na.rm = TRUE)
}

my_data <- my_data[,which(!colnames(my_data) %in% c("barcode","timepoint_id"))]
my_data <- my_data %>% filter(TB_status != "Possible")
my_data <- my_data %>% replace_with_na(replace = list(enrolment_age=0, hiv_status=common_na_strings))
my_data$TB_status[my_data$TB_status %in% c("Probable","Definite")] <- "Positive"
my_data$TB_status[my_data$TB_status != "Positive"] <- "Negative"
my_data <- my_data %>% mutate_at(c("study_site","sex","TB_status","hiv_status"), as.factor)
```

## Analyse the data
```{r plot}
plot_data <- my_data
plot_data[,7:28] <- scale(plot_data[,7:28],center=FALSE,scale=TRUE)
plot_data <- pivot_longer(plot_data,7:28, names_to="Biomarkers",values_to="Observations")
ggplot(plot_data,aes(x=TB_status,y=log10(Observations))) + geom_boxplot(outlier.size=0.3) + facet_wrap("Biomarkers",ncol=6) + ylab("log10 Concentration")
```

## Impute Data
```{r impute}
model_data <- missForest(my_data[,-1],ntree=200, decreasing=TRUE)$ximp
model_data$enrolment_age <- model_data$enrolment_age %>% floor()
```

## Build model
#Split Data
```{r split}
set.seed(1000)
stratified <- stratified(model_data, c('study_site','TB_status','sex','hiv_status'), 0.7, keep.rownames=TRUE)
sample_data <- initial_split(model_data, prop=0.7, strata = TB_status)
sample_data$in_id <- as.integer(stratified$rn)
train_data <- training(sample_data) 
test_data <- testing(sample_data)
folds <- nested_cv(train_data,
                   outside = vfold_cv(repeats = 1, strata = TB_status), 
                   inside = vfold_cv(repeats = 5, strata = TB_status))
```

#Recipes
```{r recipes}
normalised_recipe <- recipe(TB_status ~., data = train_data) %>% 
              update_role(study_site, sex, enrolment_age, new_role = "ID") %>%
              step_normalize(all_numeric_predictors()) %>%
              step_downsample(TB_status)
all_numeric_recipe <- recipe(TB_status ~., data = train_data) %>% 
              update_role(study_site, sex, enrolment_age, new_role = "ID") %>%
              step_normalize(all_numeric_predictors()) %>%
              step_dummy(all_nominal_predictors()) %>%
              step_downsample(TB_status)
```

#Models
```{r models}
#decision_tree_mod <- 
#  decision_tree(cost_complexity = tune(), tree_depth = tune()) %>% 
#  set_engine("rpart") %>% 
#  set_mode("classification") 
random_Forest_mod <- 
  rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")
logistic_regression_mod <- 
  logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")
SVM <-
  svm_linear(cost = tune(), margin = tune()) %>%
  set_engine("LiblineaR") %>%
  set_mode("classification")
# C5_O_mod <- 
#   C5_rules(trees = tune(), min_n = tune()) %>%
#   set_engine("C5.0") %>%
#   set_mode("classification")
```

#Workflows
```{r wf}
normalised_workflow <- 
  workflow_set(
    preproc = list(normalised = normalised_recipe),
    models = list(#decision_tree = decision_tree_mod, 
                  random_Forest = random_Forest_mod)) 
all_numeric_workflow <- 
  workflow_set(
    preproc = list(numeric = all_numeric_recipe),
    models = list(logistic_regression = logistic_regression_mod,
                  SVM = SVM)) 
all_workflow <- bind_rows(normalised_workflow, all_numeric_workflow) %>%
  mutate(wflow_id = gsub("(normalised_)|(numeric_)", "", wflow_id))
```

#Tuning
```{r tuning}
grid_ctrl <- control_bayes(save_pred = TRUE, save_workflow = TRUE, parallel_over = "everything")
cl <- makeCluster(8, type = "FORK")
registerDoParallel(cl)

grid_results <- foreach(i=1:1) %do% {
  all_workflow %>% workflow_map(seed = 1503,
                                fn = "tune_bayes",
                                resamples = folds$inner_resamples[[i]],
                                iter = 10,
                                parameter_info = NULL,
                                metrics = metric_set(roc_auc), 
                                objective = conf_bound(kappa=2), 
                                control = grid_ctrl)
}
stopCluster(cl)
```

## Assess training performance
```{r performance}
fit_models <- function(model, grid, data, type){
  best_result <-
    grid %>%
    collect_metrics() %>%
    filter(.metric=="roc_auc") %>%
    .[which.max(.$mean),]
  model_fit <-
    grid %>% extract_workflow(model) %>%
    finalize_workflow(best_result) %>%
    fit(data=analysis(data))
  pred <- 
    predict(model_fit, assessment(data)) %>%
    bind_cols(predict(model_fit, assessment(data), type = "prob")) %>%
    bind_cols(assessment(data) %>% select(TB_status))

  roc_auc <- pred %>% roc_auc(TB_status, .pred_Negative)
  sens <- pred %>% sensitivity(TB_status, .pred_class)
  spec <- pred %>% specificity(TB_status, .pred_class)

  perf <- tibble(Model = model, data="train") %>% 
    mutate(roc_auc = roc_auc$.estimate, sens =  sens$.estimate, spec = spec$.estimate)
    
  if (type==1){
    return(perf)
  }
  else if(type==2){
    pred <- pred %>% mutate(Resamples = data$id)
    return(pred)
  }
  else if(type==3){
    return(best_result)
  }
}
```

#Training Performance: Sens, Spec and AUC
```{r}
#model results of each fold
training_results <- foreach(x=1:10) %do% {
  model_results <- foreach(y=1:3) %do% {
    fit_models(grid_results.copy[[x]]$wflow_id[[y]], grid_results.copy[[x]]$result[[y]], folds$splits[[x]], 1)
  }
  #bind models by row
  bind_rows(model_results)
}

#get average results of each model
training_average <- foreach(x=1:3) %do% {
  bind_rows(training_results) %>% filter(Model==all_workflow$wflow_id[[x]]) %>%
    group_by(Model, data) %>% summarise_if(is.numeric, mean, na.rm=TRUE)
} %>% bind_rows()
training_average
```

#Training Performance: ROC curve
```{r}
#prediction results, with resample folds
prediction_results <- foreach(x=1:10) %do% {
  model_perf <- foreach(y=1:3) %do% {
    fit_models(grid_results.copy[[x]]$wflow_id[[y]], grid_results.copy[[x]]$result[[y]], folds$splits[[x]], 2)
  }
}

#plot average roc_curve of the 10-fold training results , with error bars
roc_curve <- foreach(x=1:3) %do% {
  model_pred <- foreach(y=1:10) %do% {
    prediction_results[[y]][[x]] %>% unnest(cols = c(Resamples)) %>% rename(Resamples=id)
  }
  bind_rows(model_pred) %>% 
    group_by(Resamples) %>% roc_curve(TB_status, .pred_Negative) %>% autoplot()
}
roc_curve
```
